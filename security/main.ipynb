{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c688ae26-bef9-4988-9133-0346dc4eff32",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5075e9a-43ff-49f5-888b-342d5ecda78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import tqdm\n",
    "import os, time, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=4, linewidth=50000)\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b68ddb-8ef2-42a1-94c1-4f708b4907d7",
   "metadata": {},
   "source": [
    "# User Define Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470d6312-f557-4f84-899f-95686b41e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '~/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c0a67-1a39-4ce3-913b-749c11f6878b",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa129a9b-9a85-4de6-b42a-f8cbfc31231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch for MNIST dataset')\n",
    "parser.add_argument('--device', type=str, default='cuda', help='Device')\n",
    "parser.add_argument('--shuffle', action='store_true', default=False, help='enables data shuffle')\n",
    "parser.add_argument('--dataset', type=str, default='mnist', help='training dataset')\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='path to MNIST')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='optimizer')\n",
    "parser.add_argument('--loss_func', type=str, default='cel', help='optimizer')\n",
    "parser.add_argument('--quant_opt', type=str, default='asym', help='Type of Quantization')\n",
    "parser.add_argument('--num_bits', type=int, default=8, help='Number of Quantization Bits')\n",
    "parser.add_argument('--act_quant', type=bool, default=False, help='Activation Quantization')\n",
    "parser.add_argument('--disp', type=bool, default=False, help='Display Model Information')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afa21c-b407-4698-bf0b-85e9dc4c94f1",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c82ac1a-0065-4d3f-a915-a01a91eb47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.device == 'cuda' else {}\n",
    "if args.dataset == 'mnist':\n",
    "\ttrain_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=True,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=transforms.ToTensor()\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)\n",
    "\n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=False,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=transforms.ToTensor()\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625782c6-b278-4485-a8c9-7c6fec5aafdf",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98354784-1637-4f2c-85b2-6b53f0805bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "\t@staticmethod\n",
    "\tdef forward(ctx, input):\n",
    "\t\tctx.save_for_backward(input)\n",
    "\t\treturn input.clamp(min=0, max=2**15)\n",
    "\t@staticmethod\n",
    "\tdef backward(ctx, grad_output):\n",
    "\t\tinput, = ctx.saved_tensors\n",
    "\t\tgrad_input = grad_output.clone()\n",
    "\t\tgrad_input[input < 0] = 0\n",
    "\t\treturn grad_input\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MLP, self).__init__()\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\t\tself.fc1 = nn.Linear(28*28, 32)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.fc2 = nn.Linear(32, 16)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\t\tself.fc3 = nn.Linear(16, 10)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.relu1(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.relu2(x)\n",
    "\t\tlogits = self.fc3(x)\n",
    "\t\treturn logits\n",
    "\t\n",
    "class LENET5(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(LENET5, self).__init__()\n",
    "\t\t#C1\n",
    "\t\tself.conv1 = nn.Conv2d(1, 6, 5)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\t#S2\n",
    "\t\tself.pool1 = nn.MaxPool2d(2)\n",
    "\t\t#C3\n",
    "\t\tself.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\t\t#S4\n",
    "\t\tself.pool2 = nn.MaxPool2d(2)\n",
    "\t\t#C5\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\t\tself.fc1 = nn.Linear(256, 120)\n",
    "\t\tself.relu3 = nn.ReLU()\n",
    "\t\t#F6\n",
    "\t\tself.fc2 = nn.Linear(120, 84)\n",
    "\t\tself.relu4 = nn.ReLU()\n",
    "\t\t#OUTPUT\n",
    "\t\tself.fc3 = nn.Linear(84, 10)\n",
    "\t\tself.relu5 = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.relu1(x)\n",
    "\t\tx = self.pool1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = self.relu2(x)\n",
    "\t\tx = self.pool2(x)\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.relu3(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.relu4(x)\n",
    "\t\tx = self.fc3(x)\n",
    "\t\tlogits = self.relu5(x)\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ebfd9aa-9e7b-4638-8db1-101093cdfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genOptimizer(model, args):\n",
    "\tif args.optimizer == 'sgd':\n",
    "\t\toptimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\tif args.optimizer == 'adam':\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\treturn optimizer\n",
    "\n",
    "def genLossFunc(args):\n",
    "\tif args.loss_func == 'cel':\n",
    "\t\tloss_func = nn.CrossEntropyLoss()\n",
    "\treturn loss_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451421ff-8f60-41fd-8a34-7922c281a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "\twith torch.no_grad():\n",
    "\t\tmaxk = max(topk)\n",
    "\t\tbatch_size = target.size(0)\n",
    "\n",
    "\t\t_, pred = output.topk(maxk, 1, True, True)\n",
    "\t\tpred = pred.t()\n",
    "\t\tcorrect = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "\t\tres = []\n",
    "\t\tfor k in topk:\n",
    "\t\t\tcorrect_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "\t\t\tres.append(correct_k.mul_(100.0 / batch_size))\n",
    "\t\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8fe687-70b1-4f29-8dd4-316acae35dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epoch, args):\n",
    "\tmodel.train()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\toptimizer = genOptimizer(model, args)\n",
    "\tmax_batch_index = int(np.floor(len(train_loader.dataset)/args.batch_size))\n",
    "\trunning_loss = 0\n",
    "\tfor batch_index, (image, label) in enumerate(tq(train_loader, desc='Train', leave=False)):\n",
    "\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\tpred = model(image)\n",
    "\t\tloss = loss_func(pred, label)\n",
    "\t\trunning_loss += loss.item()#*image.size(0)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\tprint(f'Epoch {epoch+1:<3d}: Avg. Loss: {running_loss/len(train_loader.dataset):.4f}', end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29189102-050a-463a-bbb8-18ca4ab3700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, epoch, args):\n",
    "\tmodel.eval()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\tloss, correct = 0, 0\n",
    "\twith torch.no_grad():\n",
    "# \t\tfor batch_index, (image, label) in enumerate(tq(test_loader, desc='Test', leave=False)):\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tpred = model(image)\n",
    "\t\t\tloss += loss_func(pred, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_loader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37298e4-2310-43d0-8990-500051f14cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  : Avg. Loss: 0.0084\tAccuracy: 9086/10000 (90.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  : Avg. Loss: 0.0043\tAccuracy: 9263/10000 (92.6%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  : Avg. Loss: 0.0035\tAccuracy: 9364/10000 (93.6%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4  : Avg. Loss: 0.0029\tAccuracy: 9444/10000 (94.4%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5  : Avg. Loss: 0.0025\tAccuracy: 9494/10000 (94.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6  : Avg. Loss: 0.0023\tAccuracy: 9534/10000 (95.3%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7  : Avg. Loss: 0.0020\tAccuracy: 9549/10000 (95.5%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  : Avg. Loss: 0.0018\tAccuracy: 9569/10000 (95.7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9  : Avg. Loss: 0.0017\tAccuracy: 9577/10000 (95.8%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 : Avg. Loss: 0.0016\tAccuracy: 9597/10000 (96.0%)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def main(model):\n",
    "\tfor epoch in range(args.epochs):\n",
    "\t\ttrain(train_loader, model, epoch, args)\n",
    "\t\ttest(test_loader, model, epoch, args)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model\n",
    "\n",
    "model = main(MLP().to(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9954dbb-5161-4e77-a78c-8811b014d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScaleZeroPoint(min_val, max_val, args):\n",
    "\tqmin, qmax = 0., 2.**args.num_bits - 1.\n",
    "\tif args.quant_opt == 'asym':\n",
    "\t\tscale = (max_val - min_val) / (qmax - qmin)\n",
    "\t\tif scale is not 0.:\n",
    "\t\t\tscale = 2**round(math.log(abs(scale), 2))\n",
    "\t\tinitial_zero_point = -min_val / scale\n",
    "\t\tzero_point = int({initial_zero_point < qmin: qmin,  initial_zero_point > qmax: qmax, qmin <= initial_zero_point <= qmax: initial_zero_point}.get(True, False))\n",
    "\tif args.quant_opt == 'sym':\n",
    "\t\tscale = max(abs(min_val), abs(max_val)) / qmax\n",
    "\t\tzero_point = 0\n",
    "\treturn scale, zero_point\n",
    "\n",
    "def quantizeTensor(input_tensor, min_val, max_val, args):\n",
    "\tif not min_val and not max_val:\n",
    "\t\tmin_val, max_val = input_tensor.min(), input_tensor.max()\n",
    "\t\n",
    "\tqmin, qmax = 0., 2.**args.num_bits - 1.\n",
    "\tscale, zero_point = calcScaleZeroPoint(min_val, max_val, args)\n",
    "\t\n",
    "\tif args.quant_opt == 'asym':\n",
    "\t\tquant_tensor = (input_tensor / scale + zero_point).clamp(qmin, qmax).round()\n",
    "\tif args.quant_opt == 'sym':\n",
    "\t\tquant_tensor = (input_tensor / scale).clamp(qmax, qmax).round()\n",
    "\treturn \tdict(zip(['qTensor', 'scale', 'zero_point'], [quant_tensor, scale, zero_point]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0df0d2-9ffa-4e30-861a-fc9e25d77a27",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "r = S(q-Z) \\\\\n",
    "here, r: real-value, S: scale, q: quantized-value, Z: zero-point\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "805346c4-b07b-4c8c-b01b-c2780b475a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantizeTensor(qDict, args):\n",
    "\tif args.quant_opt == 'asym':\n",
    "\t\tdequant_tensor = qDict['scale'] * (qDict['qTensor'].float() - qDict['zero_point'])\n",
    "\tif args.quant_opt == 'sym':\n",
    "\t\tdequant_tensor = qDict['scale'] * (qDict['qTensor'].float())\t\n",
    "\treturn dequant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "298d8275-94e1-46c9-a348-6777c737ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateStats(actTensor, stats, layerName):\n",
    "\t# dim=0 : find min/max in each col\n",
    "\t# dim=1 : find min/max in each row\n",
    "\tmaxValue = torch.max(actTensor, dim=1)[0]\n",
    "\tminValue = torch.min(actTensor, dim=1)[0]\n",
    "\t\n",
    "\tif layerName not in stats:\n",
    "\t\tstats[layerName] = {'max': maxValue.sum(), 'min': minValue.sum(), 'total': 1}\n",
    "\telse:\n",
    "\t\tstats[layerName]['max'] += maxValue.sum().item()\n",
    "\t\tstats[layerName]['min'] += maxValue.sum().item()\n",
    "\t\tstats[layerName]['total'] += 1\n",
    "\t\t\n",
    "\tweighting = 2.0 / (stats[layerName]['total']) + 1\n",
    "\t\n",
    "\tif 'ema_min' in stats[layerName]:\n",
    "\t\tstats[layerName]['ema_min'] = weighting*(minValue.mean().item()) + (1 - weighting) * stats[layerName]['ema_min']\n",
    "\telse:\n",
    "\t\tstats[layerName]['ema_min'] = weighting*(minValue.mean().item())\n",
    "\n",
    "\tif 'ema_max' in stats[layerName]:\n",
    "\t\tstats[layerName]['ema_max'] = weighting*(maxValue.mean().item()) + (1 - weighting) * stats[layerName]['ema_max']\n",
    "\telse: \n",
    "\t\tstats[layerName]['ema_max'] = weighting*(maxValue.mean().item())\n",
    "\treturn stats\n",
    "\n",
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "\tx = model.flatten(x)\n",
    "\tstats = updateStats(x, stats, 'fc1')\n",
    "\tx = F.relu(model.fc1(x))\n",
    "\t\n",
    "\tstats = updateStats(x, stats, 'fc2')\n",
    "\tx = F.relu(model.fc2(x))\n",
    "\n",
    "\tstats = updateStats(x, stats, 'fc3')\n",
    "# \tx = model.fc3(x)\n",
    "\treturn stats\n",
    "\n",
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, test_loader, args):\n",
    "\tmodel.eval()\n",
    "\tstats = {}\n",
    "\twith torch.no_grad():\n",
    "\t\tfor image, label in test_loader:\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tstats = gatherActivationStats(model, image, stats)\n",
    "\n",
    "\tfinal_stats = {}\n",
    "\tfor key, value in stats.items():\n",
    "\t\tfinal_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
    "\treturn final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29c589c5-7ca6-4a54-ab0a-ad637975dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, min_val, max_val, args):\n",
    "\t\t#def quantizeTensor(input_tensor, min_val=None, max_val=None, num_bits=4, opt='asym'):\n",
    "        x = quantizeTensor(x, min_val, max_val, args)\n",
    "        x = dequantizeTensor(x, args)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # straight through estimator\n",
    "        return grad_output, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e137463-19bd-4209-8883-cfaae075f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantAwareTrainingForward(x, model, stats, args):\n",
    "\twith torch.no_grad():\n",
    "\t\tparams = model.state_dict()\n",
    "# \t\tlayer_names, _ = zip(*list(model.named_modules())[1:])\n",
    "\n",
    "\tx = model.flatten(x)\n",
    "\t\n",
    "\tmodel.fc1.weight.data = FakeQuantOp.apply(model.fc1.weight.data, None, None, args)\n",
    "\tmodel.fc1.bias.data = FakeQuantOp.apply(model.fc1.bias.data, None, None, args)\n",
    "\n",
    "# \tx = F.relu(model.fc1(x))\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc1')\n",
    "\tif args.act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc1']['ema_min'], stats['fc1']['ema_max'], args)\n",
    "# \tx = model.relu1(model.fc1(x))\n",
    "\trelu1 = MyReLU.apply\n",
    "\tx = relu1(model.fc1(x))\n",
    "\t\n",
    "\tmodel.fc2.weight.data = FakeQuantOp.apply(model.fc2.weight.data, None, None, args)\n",
    "\tmodel.fc2.bias.data = FakeQuantOp.apply(model.fc2.bias.data, None, None, args)\n",
    "# \tx = F.relu(model.fc2(x))\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc2')\n",
    "\tif args.act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc2']['ema_min'], stats['fc2']['ema_max'], args)\n",
    "# \tx = model.relu2(model.fc2(x))\n",
    "\trelu2 = MyReLU.apply\n",
    "\tx = relu2(model.fc2(x))\n",
    "\t\n",
    "\tmodel.fc3.weight.data = FakeQuantOp.apply(model.fc3.weight.data, None, None, args)\n",
    "\tmodel.fc3.bias.data = FakeQuantOp.apply(model.fc3.bias.data, None, None, args)\n",
    "# \tx = model.fc3(x)\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc3')\n",
    "\tif args.act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc3']['ema_min'], stats['fc3']['ema_max'], args)\n",
    "\tx = model.fc3(x)\t\n",
    "\treturn x, params, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b5cabcd-e6c1-4e0b-8657-518ccd2238b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQAT(train_loader, model, epoch, stats, args):\n",
    "\tmodel.train()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\toptimizer = genOptimizer(model, args)\n",
    "\tmax_batch_index = int(np.floor(len(train_loader.dataset)/args.batch_size))\n",
    "\trunning_loss = 0 \n",
    "\tfor batch_index, (image, label) in enumerate(tq(train_loader, desc='Train', leave=False)):\n",
    "\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ty = model(image)\n",
    "\t\ty, params, stats = quantAwareTrainingForward(image, model, stats, args)\n",
    "\t\t\n",
    "\t\t# Recover FP32 to improve accuracy\n",
    "\t\t# \t\tmodel.fc1.weight.data = params['fc1.weight']\n",
    "\t\tfor param, _ in model.named_parameters():\n",
    "\t\t\texec(f'model.{param}.data = params[\"{param}\"]')\n",
    "\n",
    "\t\tloss = loss_func(y, label)\t\t\n",
    "\t\trunning_loss += loss.item()#*image.size(0)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\t\n",
    "\t\tif batch_index == max_batch_index:\n",
    "\t\t\tprint(f'[Epoch {epoch+1:>2d}] Training Loss: {running_loss/len(train_loader.dataset):.4f}', end = ' / ')\n",
    "\t\t\t\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef53312f-b91a-424b-bbb9-ca0fd6221e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQAT(test_loader, model, epoch, stats, args):\n",
    "\tmodel.eval()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\tloss, correct = 0, 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\ty, _, _ = quantAwareTrainingForward(image, model, stats, args)\t\n",
    "\t\t\tloss += loss_func(y, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_loader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Test Loss:  {loss:>.4f} / Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22d92e-c0b9-44a8-91f3-e2813dd43abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] Training Loss: 0.0142 / Test Loss:  0.0073 / Accuracy: 8666/10000 (86.7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2] Training Loss: 0.0068 / Test Loss:  0.0061 / Accuracy: 8868/10000 (88.7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c647290d124c13b09650007a7092e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mainQAT(model):\n",
    "\tstats = {}\n",
    "\tfor epoch in range(args.epochs):\n",
    "\t\targs.act_quant = True if epoch > 5 else False\n",
    "\t\tstats = trainQAT(train_loader, model, epoch, stats, args)\n",
    "\t\ttestQAT(test_loader, model, epoch, stats, args)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model, stats\n",
    "\n",
    "q_model, old_stats = mainQAT(MLP().to(args.device))\n",
    "# q_model, old_stats = mainQAT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb7864-cdb1-43bb-8b1e-f3376692a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQAT(test_loader, q_model, epoch=10, stats=old_stats, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c81318-42b1-4f11-bc27-538414ef23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeLayer(layer, x, args):\n",
    "\twDict = quantizeTensor(layer.weight.data, None, None, args)\n",
    "\tbDict = quantizeTensor(layer.bias.data, None, None, args)\n",
    "\tlayer.weight.data = wDict['qTensor']\n",
    "\tlayer.bias.data = bDict['qTensor']\n",
    "\t\n",
    "\txDict = quantizeTensor(x, None, None, args)\n",
    "\toActDict = quantizeTensor(layer(xDict['qTensor']), None, None, args)\n",
    "\treturn oActDict\n",
    "\n",
    "def quantForward(model, x, args):\n",
    "\tx = model.flatten(x)\n",
    "\t# Input\n",
    "\tactDict0 = quantizeTensor(x, None, None, args)\n",
    "\t# 1st FC Layer\n",
    "\tactDict1 = quantizeLayer(model.fc1, actDict0['qTensor'], args)\n",
    "\tact1 = model.relu1(actDict1['qTensor'])\n",
    "\tactDict2 = quantizeLayer(model.fc2, act1, args)\n",
    "\tact2 = model.relu2(actDict2['qTensor'])\n",
    "\tactDict3 = quantizeLayer(model.fc3, act2, args)\n",
    "\treturn actDict0, actDict1, actDict2, actDict3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2efcf-ddd4-48ad-bb9b-606afe436ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQuant(model, test_loader, stats, quant, args):\n",
    "\tmodel.eval()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\tloss, correct = 0, 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tif quant:\n",
    "\t\t\t\tactDict0, actDict1, actDict2, actDict3  = quantForward(model, image, args)\n",
    "\t\t\t\ty = actDict3['qTensor']\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = model(image)\n",
    "\t\t\tloss += loss_func(y, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%) Loss: {loss/len(test_loader.dataset):.2f}')\n",
    "\treturn actDict0, actDict1, actDict2, actDict3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d34ad-e66e-4724-9fc3-bd6207145924",
   "metadata": {},
   "outputs": [],
   "source": [
    "actDict0, actDict1, actDict2, actDict3 = testQuant(q_model, test_loader, old_stats, True, args)\n",
    "# testQuant(MLP().to(args.device), test_loader, old_stats, True, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688b07e-3d44-4afd-8efd-c9e6aeef15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actDict0['qTensor'][15].view(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dd357-b074-4aed-b2a8-8a88e8d51650",
   "metadata": {},
   "outputs": [],
   "source": [
    "for actDict in [actDict0, actDict1, actDict2, actDict3]:\n",
    "\tprint(actDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64faa516-db53-4ca4-9194-5e6fcf9547b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizeTensor(q_model.fc1(actDict0['qTensor']), None, None, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583fd3a6-b9a0-44d6-b3bc-4699bef35c5e",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d5345d-ac54-4845-9be1-bff83185cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a34062-b6ae-4e07-bbed-658d7c05525d",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b725318-c76b-45da-a04f-e2ebac8b9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '~/dataset'\n",
    "train_data = datasets.MNIST(root=dataset_dir, train=True,  download=True, transform=ToTensor())\n",
    "test_data  = datasets.MNIST(root=dataset_dir, train=False, download=True, transform=ToTensor())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Number of trainData/validData/testData = 50000/10000/10000\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_data,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdddba9e-3d46-49bd-8d02-e82b4b81999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(x, axs):\n",
    "\tx = x.view(-1).cpu().numpy()\n",
    "\taxs.hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d9254-d9cf-4f28-b95d-92fa7df16fc7",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccd9bff-19b3-45df-a200-20ca50454eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "class NN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(NN, self).__init__()\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\t\tself.fc1 = nn.Linear(28*28, 16)\n",
    "\t\tself.fc2 = nn.Linear(16, 16)\n",
    "\t\tself.fc3 = nn.Linear(16, 10)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tlogits = self.fc3(x)\n",
    "\n",
    "\t\treturn logits\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b664c-17c4-4d34-ba3b-93497aa44698",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80be6dd-b33a-4fb5-85d0-c32218d8d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, model, loss_func, optimizer, epoch):\n",
    "\tmodel.train()\n",
    "\tmax_batch_index = int(np.floor(len(train_data)/batch_size))\n",
    "\tfor batch_index, (image, label) in enumerate(train_dataloader):\n",
    "\t\timage, label = image.to(device), label.to(device)\n",
    "\t\tpred = model(image)\n",
    "\t\tloss = loss_func(pred, label)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\tif batch_index == max_batch_index:\n",
    "\t\t\tprint(f'Epoch {epoch+1:<3d}: Loss: {loss.item():.2f}', end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079efc4f-7722-431f-86d4-c63b94bcebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataloader, model, loss_func, epoch):\n",
    "\tmodel.eval()\n",
    "\tloss, correct = 0, 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor image, label in test_dataloader:\n",
    "\t\t\timage, label = image.to(device), label.to(device)\n",
    "\t\t\tpred = model(image)\n",
    "\t\t\tloss += loss_func(pred, label).item()\n",
    "\t\t\tcorrect += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_dataloader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_dataloader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_data)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6c6141-94fe-4ee6-ad1e-4a59fe1db8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\tmodel = NN().to(device)\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\tepochs = 10\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttrain(train_dataloader, model, loss_func, optimizer, epoch)\n",
    "\t\ttest(test_dataloader, model, loss_func, epoch)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model\n",
    "\n",
    "# model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01350b1-661b-44f3-bd40-9d6f2eb91552",
   "metadata": {},
   "source": [
    "# Quantization of Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b732b0-6fd7-4714-aa25-c79dfbba54b7",
   "metadata": {},
   "source": [
    "## Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40788c28-5892-4c83-b4cd-85fe48720d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScaleZeroPoint(min_val, max_val, num_bits=8, opt='asym'):\n",
    "\tqmin, qmax = 0., 2.**num_bits - 1.\n",
    "\tif opt == 'asym':\n",
    "\t\tscale = (max_val - min_val) / (qmax - qmin)\n",
    "\t\tinitial_zero_point = -min_val / scale\n",
    "\t\tzero_point = int({initial_zero_point < qmin: qmin,  initial_zero_point > qmax: qmax, qmin <= initial_zero_point <= qmax: initial_zero_point}.get(True, False))\n",
    "\tif opt == 'sym':\n",
    "\t\tscale = max(abs(min_val), abs(max_val)) / qmax\n",
    "\t\tzero_point = 0\n",
    "\treturn scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a8f4aa-1f98-4f3b-a11a-bbec3ca99868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeTensor(input_tensor, min_val=None, max_val=None, num_bits=8, opt='asym'):\n",
    "\tif not min_val and not max_val:\n",
    "\t\tmin_val, max_val = input_tensor.min(), input_tensor.max()\n",
    "\t\n",
    "\tqmin, qmax = 0., 2.**num_bits - 1.\n",
    "\tscale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits, opt)\n",
    "\t\n",
    "\tif opt == 'asym':\n",
    "\t\tquant_tensor = (input_tensor / scale + zero_point).clamp(qmin, qmax).round()\n",
    "\tif opt == 'sym':\n",
    "\t\tquant_tensor = (input_tensor / scale).clamp(qmax, qmax).round()\n",
    "\t\n",
    "\tqTuple = namedtuple('qTuple', ['tensor', 'scale', 'zero_point'])\n",
    "\treturn qTuple(tensor=quant_tensor, scale=scale, zero_point=zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "551f3c25-e727-4434-a601-f68e6e0cafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantizeTensor(qTuple, opt='asym'):\n",
    "\tif opt == 'asym':\n",
    "\t\tdequant_tensor = qTuple.scale * (qTuple.tensor.float() - qTuple.zero_point)\n",
    "\tif opt == 'sym':\n",
    "\t\tdequant_tensor = qTuple.scale * (qTuple.tensor.float())\t\n",
    "\treturn dequant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b7e09c-4523-44da-bad9-44f53c86f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateStats(actTensor, stats, layerName):\n",
    "\t# dim=0 : find min/max in each col\n",
    "\t# dim=1 : find min/max in each row\n",
    "\tmaxValue = torch.max(actTensor, dim=1)[0]\n",
    "\tminValue = torch.min(actTensor, dim=1)[0]\n",
    "\t\n",
    "\tif layerName not in stats:\n",
    "\t\tstats[layerName] = {'max': maxValue.sum(), 'min': minValue.sum(), 'total': 1}\n",
    "\telse:\n",
    "\t\tstats[layerName]['max'] += maxValue.sum().item()\n",
    "\t\tstats[layerName]['min'] += maxValue.sum().item()\n",
    "\t\tstats[layerName]['total'] += 1\n",
    "\t\t\n",
    "\tweighting = 2.0 / (stats[layerName]['total']) + 1\n",
    "\n",
    "\tif 'ema_min' in stats[layerName]:\n",
    "\t\tstats[layerName]['ema_min'] = weighting*(minValue.mean().item()) + (1 - weighting) * stats[layerName]['ema_min']\n",
    "\telse:\n",
    "\t\tstats[layerName]['ema_min'] = weighting*(minValue.mean().item())\n",
    "\n",
    "\tif 'ema_max' in stats[layerName]:\n",
    "\t\tstats[layerName]['ema_max'] = weighting*(maxValue.mean().item()) + (1 - weighting) * stats[layerName]['ema_max']\n",
    "\telse: \n",
    "\t\tstats[layerName]['ema_max'] = weighting*(maxValue.mean().item())\n",
    "\n",
    "\t\tstats[layerName]['min_val'] = stats[layerName]['min']/ stats[layerName]['total']\n",
    "\t\tstats[layerName]['max_val'] = stats[layerName]['max']/ stats[layerName]['total']\n",
    "\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af59c4ae-3014-4e64-af00-c92eeb54b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "\tx = model.flatten(x)\n",
    "\tstats = updateStats(x, stats, 'fc1')\n",
    "\tx = F.relu(model.fc1(x))\n",
    "\t\n",
    "\tstats = updateStats(x, stats, 'fc2')\n",
    "\tx = F.relu(model.fc2(x))\n",
    "\n",
    "\tstats = updateStats(x, stats, 'fc3')\n",
    "\tx = model.fc3(x)\n",
    "\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4865518a-d7ca-482d-a6e4-38cb8b9459e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, testDataLoader):\n",
    "\tmodel.eval()\n",
    "\tstats = {}\n",
    "\twith torch.no_grad():\n",
    "\t\tfor img, lab in testDataLoader:\n",
    "\t\t\timg, lab = img.to(device), lab.to(device)\n",
    "\t\t\tstats = gatherActivationStats(model, img, stats)\n",
    "\n",
    "\tfinal_stats = {}\n",
    "\tfor key, value in stats.items():\n",
    "\t\tfinal_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
    "\treturn final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0986c97-c07a-4974-a2a2-7fc7e3eb2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, min_val=None, max_val=None, num_bits=8, opt='asym'):\n",
    "\t\t#def quantizeTensor(input_tensor, min_val=None, max_val=None, num_bits=4, opt='asym'):\n",
    "        x = quantizeTensor(x, min_val=min_val, max_val=max_val, num_bits=num_bits, opt=opt)\n",
    "        x = dequantizeTensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # straight through estimator\n",
    "        return grad_output, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebd77813-e820-4f02-8feb-64bc5cd71b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantAwareTrainingForward(x, model, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\tparams = model.state_dict()\n",
    "\t\n",
    "\tx = model.flatten(x)\n",
    "\t\n",
    "\tmodel.fc1.weight.data = FakeQuantOp.apply(model.fc1.weight.data, None, None, num_bits, opt)\n",
    "\tmodel.fc1.bias.data = FakeQuantOp.apply(model.fc1.bias.data, None, None, num_bits, opt)\n",
    "\n",
    "# \tx = F.relu(model.fc1(x))\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc1')\n",
    "\tif act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc1']['ema_min'], stats['fc1']['ema_max'], num_bits, opt)\n",
    "\tx = F.relu(model.fc1(x))\n",
    "\t\n",
    "\tmodel.fc2.weight.data = FakeQuantOp.apply(model.fc2.weight.data, None, None, num_bits, opt)\n",
    "\tmodel.fc2.bias.data = FakeQuantOp.apply(model.fc2.bias.data, None, None, num_bits, opt)\n",
    "# \tx = F.relu(model.fc2(x))\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc2')\n",
    "\tif act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc2']['ema_min'], stats['fc2']['ema_max'], num_bits, opt)\n",
    "\tx = F.relu(model.fc2(x))\n",
    "\n",
    "\tmodel.fc3.weight.data = FakeQuantOp.apply(model.fc3.weight.data, None, None, num_bits, opt)\n",
    "\tmodel.fc3.bias.data = FakeQuantOp.apply(model.fc3.bias.data, None, None, num_bits, opt)\n",
    "# \tx = model.fc3(x)\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc3')\n",
    "\tif act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc3']['ema_min'], stats['fc3']['ema_max'], num_bits, opt)\n",
    "\tx = model.fc3(x)\n",
    "\t\n",
    "#\treturn F.log_softmax(x, dim=1), fc1_weight, fc2_weight, fc3_weight, stats\n",
    "\treturn x, params, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1801eb1c-e0df-473f-971f-8c8943c04f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQAT(train_dataloader, model, loss_func, optimizer, epoch, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\tmodel.train()\n",
    "\tmax_batch_index = int(np.floor(len(train_data)/batch_size))\n",
    "\tfor batch_index, (image, label) in enumerate(train_dataloader):\n",
    "\t\timage, label = image.to(device), label.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ty = model(image)\n",
    "\t\ty, params, stats = quantAwareTrainingForward(image, model, stats, act_quant, num_bits, opt)\n",
    "\t\t\n",
    "\t\t# Recover FP32 to improve accuracy\n",
    "\t\tmodel.fc1.weight.data = params['fc1.weight']\n",
    "\t\tmodel.fc2.weight.data = params['fc2.weight']\n",
    "\t\tmodel.fc3.weight.data = params['fc3.weight']\n",
    "\t\tmodel.fc1.bias.data   = params['fc1.bias']\n",
    "\t\tmodel.fc2.bias.data   = params['fc2.bias']\n",
    "\t\tmodel.fc3.bias.data   = params['fc3.bias']\n",
    "\n",
    "\t\tloss = loss_func(y, label)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\tif batch_index == max_batch_index:\n",
    "\t\t\tprint(f'Epoch {epoch+1:<3d}: Loss: {loss.item():.2f}', end = '\\t')\n",
    "\t\t\t\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16fb3bab-b0f9-4515-80fe-bbb156cbc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQAT(test_dataloader, model, epoch, loss_func, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\tmodel.eval()\n",
    "\tloss, correct = 0, 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor image, label in test_dataloader:\n",
    "\t\t\timage, label = image.to(device), label.to(device)\n",
    "\t\t\ty, _, _ = quantAwareTrainingForward(image, model, stats, act_quant, num_bits, opt)\t\n",
    "\t\t\tloss += loss_func(y, label).item()\n",
    "\t\t\tcorrect += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_dataloader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_dataloader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_data)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "951d6389-bfeb-4828-8dec-563dc75dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainQAT():\n",
    "\tmodel = NN().to(device)\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\tepochs = 10\n",
    "\tnum_bits = 16\n",
    "\tstats = {}\n",
    "\topt = 'asym'\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tact_quant = True if epoch > 5 else False\n",
    "\t\t#def trainQAT(train_dataloader, model, loss_func, optimizer, epoch, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\t\tstats = trainQAT(train_dataloader, model, loss_func, optimizer, epoch, stats, act_quant, num_bits, opt)\n",
    "\t\ttestQAT(test_dataloader, model, epoch, loss_func, stats, act_quant, num_bits, opt)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87681f61-fcf6-4799-985b-5f45d420bc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  : Loss: 2.12\tAccuracy: 2971/10000 (29.7%)\n",
      "Epoch 2  : Loss: 1.23\tAccuracy: 6480/10000 (64.8%)\n",
      "Epoch 3  : Loss: 0.56\tAccuracy: 7841/10000 (78.4%)\n",
      "Epoch 4  : Loss: 0.38\tAccuracy: 8402/10000 (84.0%)\n",
      "Epoch 5  : Loss: 0.31\tAccuracy: 8633/10000 (86.3%)\n",
      "Epoch 6  : Loss: 0.27\tAccuracy: 8740/10000 (87.4%)\n",
      "Epoch 7  : Loss: 0.35\tAccuracy: 8196/10000 (82.0%)\n",
      "Epoch 8  : Loss: 0.32\tAccuracy: 8333/10000 (83.3%)\n",
      "Epoch 9  : Loss: 0.53\tAccuracy: 7996/10000 (80.0%)\n",
      "Epoch 10 : Loss: 0.32\tAccuracy: 8413/10000 (84.1%)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "q_model, old_stats = mainQAT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aff51b-eec0-4568-ba99-55daa00f9a89",
   "metadata": {},
   "source": [
    "## Rework Forward pass of Linear and Conv Layers to support Quantisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b011b-7872-49ed-8277-41448829cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQAT(test_dataloader, q_model, epoch=10, loss_func=loss_func, stats=old_stats, act_quant=True, num_bits=4, opt='asym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac1184-ea39-4ff6-b612-7de355a959c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = q_model.state_dict()['fc1.weight']\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625529d-2395-455e-99f0-08e96b542412",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = quantizeTensor(weight)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6b3d8-d10b-4604-b537-c3ae1015609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantizeTensor(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fcaa9a-207f-4578-9899-b8745615549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75517c42-e301-488b-9b28-098823b53667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeLayer(qActTuple, layer, stat, numBits=4, opt='asym'):\n",
    "\tW, B = layer.weight.data, layer.bias.data\n",
    "\t\n",
    "\tqWeightTuple = quantizeTensor(W, numBits=numBits, opt=opt)\n",
    "\tqBiasTuple   = quantizeTensor(B, numBits=numBits, opt=opt)\n",
    "\t\n",
    "\tlayer.weight.data = qWeightTuple.qTensor.float()\n",
    "\tlayer.bias.data   = qBiasTuple.qTensor.float()\n",
    "\t\n",
    "\tnextScale, nextZeroPoint = calcScaleZeroPoint(minValue=stat['min'], maxValue=stat['max'], numBits=numBits, opt=opt)\n",
    "\t\n",
    "\tweightScale = qWeightTuple.scale\n",
    "\tweightZeroPoint = qWeightTuple.zeroPoint\n",
    "\t\n",
    "\tif opt == 'asym':\n",
    "\t\tlayer.weight.data = ((qWeightTuple.scale * qActTuple.scale) / nextScale) * (layer.weight.data - qWeightTuple.zeroPoint)\n",
    "\t\tlayer.bias.data = (qBiasTuple.scale / nextScale) * (layer.bias.data - qBiasTuple.zeroPoint)\t\t\n",
    "\t\toAct = layer(qActTuple.qTensor.float() - qActTuple.zeroPoint) + nextZeroPoint\n",
    "\tif opt == 'sym':\n",
    "\t\tlayer.weight.data = ((qWeightTuple.scale * qActTuple.scale) / nextScale) * (layer.weight.data)\n",
    "\t\tlayer.bias.data = (qBiasTuple.scale / nextScale) * (layer.bias.data)\n",
    "\t\toAct = layer(qActTuple.qTensor.float())\n",
    "\t\t\n",
    "\tlayer.weight.data, layer.bias.data = W, B\n",
    "\t\n",
    "\treturn oAct.round(), nextScale, nextZeroPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b34509-aadf-4e11-9c60-7495f367f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantForward(x, model, stats, num_bits=8, opt='asym'):\n",
    "\t# Quantise before inputting into incoming layers\n",
    "\tx = quantizeTensor()\n",
    "\tif sym:\n",
    "\t\tx = quantize_tensor_sym(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "\telse:\n",
    "\t\tx = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "\n",
    "\tx, scale_next, zero_point_next = quantizeLayer(x.tensor, model.conv1, stats['conv2'], x.scale, x.zero_point, vis, axs, X=X, y=y+1, sym=sym, num_bits=num_bits)\n",
    "\n",
    "\tx = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "\n",
    "\n",
    "\tx, scale_next, zero_point_next = quantizeLayer(x, model.conv2, stats['fc1'], scale_next, zero_point_next, vis, axs, X=X, y=y+3, sym=sym, num_bits=num_bits)\n",
    "\n",
    "\tx = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "\n",
    "\tx = x.view(-1, 4*4*50)\n",
    "\n",
    "\tx, scale_next, zero_point_next = quantizeLayer(x, model.fc1, stats['fc2'], scale_next, zero_point_next, vis, axs, X=X+1, y=0, sym=sym, num_bits=num_bits)\n",
    "\n",
    "\n",
    "\t# Back to dequant for final layer\n",
    "# \tif sym:\n",
    "# \t\tx = dequantize_tensor_sym(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "# \telse:\n",
    "# \t\tx = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "\n",
    "\n",
    "\tx = model.fc2(x)\n",
    "\n",
    "\tif vis:\n",
    "\t\taxs[X+1,3].set_xlabel('Unquantised Weights of fc2 layer')\n",
    "\t\tvisualise(model.fc2.weight.data,axs[X+1,3])\n",
    "\n",
    "\t\taxs[X+1,2].set_xlabel('Output after fc2 but dequantised visualised below: ')\n",
    "\t\tvisualise(x,axs[X+1,4])\n",
    "\n",
    "\treturn F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ae37e-fa48-4b07-99bf-b71a0780733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73fe8b-b293-4f7a-a92c-dd187a4cf2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a4f38-03c2-480f-9939-01adc1397946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583fd3a6-b9a0-44d6-b3bc-4699bef35c5e",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d5345d-ac54-4845-9be1-bff83185cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a34062-b6ae-4e07-bbed-658d7c05525d",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b725318-c76b-45da-a04f-e2ebac8b9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '~/dataset'\n",
    "train_data = datasets.MNIST(root=dataset_dir, train=True,  download=True, transform=ToTensor())\n",
    "test_data  = datasets.MNIST(root=dataset_dir, train=False, download=True, transform=ToTensor())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Number of trainData/validData/testData = 50000/10000/10000\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_data,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdddba9e-3d46-49bd-8d02-e82b4b81999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(x, axs):\n",
    "\tx = x.view(-1).cpu().numpy()\n",
    "\taxs.hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d9254-d9cf-4f28-b95d-92fa7df16fc7",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccd9bff-19b3-45df-a200-20ca50454eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "class NN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(NN, self).__init__()\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\t\tself.fc1 = nn.Linear(28*28, 16)\n",
    "\t\tself.fc2 = nn.Linear(16, 16)\n",
    "\t\tself.fc3 = nn.Linear(16, 10)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tlogits = self.fc3(x)\n",
    "\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b664c-17c4-4d34-ba3b-93497aa44698",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80be6dd-b33a-4fb5-85d0-c32218d8d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, model, loss_func, optimizer, epoch):\n",
    "\tmodel.train()\n",
    "\tmax_batch_index = int(np.floor(len(train_data)/batch_size))\n",
    "\tfor batch_index, (image, label) in enumerate(train_dataloader):\n",
    "\t\timage, label = image.to(device), label.to(device)\n",
    "\t\tpred = model(image)\n",
    "\t\tloss = loss_func(pred, label)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\tif batch_index == max_batch_index:\n",
    "\t\t\tprint(f'Epoch {epoch+1:<3d}: Loss: {loss.item():.2f}', end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079efc4f-7722-431f-86d4-c63b94bcebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataloader, model, loss_func, epoch):\n",
    "\tmodel.eval()\n",
    "\tloss, correct = 0, 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor image, label in test_dataloader:\n",
    "\t\t\timage, label = image.to(device), label.to(device)\n",
    "\t\t\tpred = model(image)\n",
    "\t\t\tloss += loss_func(pred, label).item()\n",
    "\t\t\tcorrect += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_dataloader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_dataloader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_data)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c6141-94fe-4ee6-ad1e-4a59fe1db8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\tmodel = NN().to(device)\n",
    "\tloss_func = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\tepochs = 5\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttrain(train_dataloader, model, loss_func, optimizer, epoch)\n",
    "\t\ttest(test_dataloader, model, loss_func, epoch)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model\n",
    "\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01350b1-661b-44f3-bd40-9d6f2eb91552",
   "metadata": {},
   "source": [
    "# Quantization of Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b732b0-6fd7-4714-aa25-c79dfbba54b7",
   "metadata": {},
   "source": [
    "## Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40788c28-5892-4c83-b4cd-85fe48720d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScaleZeroPoint(min_val, max_val, num_bits=8, opt='asym'):\n",
    "\tqmin, qmax = 0., 2.**num_bits - 1.\n",
    "\tif opt == 'asym':\n",
    "\t\tscale = (max_val - min_val) / (qmax - qmin)\n",
    "\t\tinitial_zero_point = -min_val / scale\n",
    "\t\tzero_point = int({initial_zero_point < qmin: qmin,  initial_zero_point > qmax: qmax, qmin <= initial_zero_point <= qmax: initial_zero_point}.get(True, False))\n",
    "\tif opt == 'sym':\n",
    "\t\tscale = max(abs(min_val), abs(max_val)) / qmax\n",
    "\t\tzero_point = 0\n",
    "\treturn scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a8f4aa-1f98-4f3b-a11a-bbec3ca99868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeTensor(input_tensor, min_val=None, max_val=None, num_bits=8, opt='asym'):\n",
    "\tif not min_val and not max_val:\n",
    "\t\tmin_val, max_val = input_tensor.min(), input_tensor.max()\n",
    "\t\n",
    "\tqmin, qmax = 0., 2.**num_bits - 1.\n",
    "\tscale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits, opt)\n",
    "\t\n",
    "\tif opt == 'asym':\n",
    "\t\tquant_tensor = (input_tensor / scale + zero_point).clamp(qmin, qmax).round()\n",
    "\tif opt == 'sym':\n",
    "\t\tquant_tensor = (input_tensor / scale).clamp(qmax, qmax).round()\n",
    "\t\n",
    "\tqTuple = namedtuple('qTuple', ['tensor', 'scale', 'zero_point'])\n",
    "\treturn qTuple(tensor=quant_tensor, scale=scale, zero_point=zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "551f3c25-e727-4434-a601-f68e6e0cafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantizeTensor(qTuple, opt='asym'):\n",
    "\tif opt == 'asym':\n",
    "\t\tdequant_tensor = qTuple.scale * (qTuple.tensor.float() - qTuple.zero_point)\n",
    "\tif opt == 'sym':\n",
    "\t\tdequant_tensor = qTuple.scale * (qTuple.tensor.float())\t\n",
    "\treturn dequant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b7e09c-4523-44da-bad9-44f53c86f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateStats(actTensor, stats, layerName):\n",
    "\t# dim=0 : find min/max in each col\n",
    "\t# dim=1 : find min/max in each row\n",
    "\tmaxValue = torch.max(actTensor, dim=1)[0]\n",
    "\tminValue = torch.min(actTensor, dim=1)[0]\n",
    "\t\n",
    "\tif layerName not in stats:\n",
    "\t\tstats[layerName] = {'max': maxValue.sum(), 'min': minValue.sum(), 'total': 1}\n",
    "\telse:\n",
    "\t\tstats[layerName]['max'] += maxValue.sum().item()\n",
    "\t\tstats[layerName]['min'] += maxValue.sum().item()\n",
    "\t\tstats[layerName]['total'] += 1\n",
    "\t\t\n",
    "\tweighting = 2.0 / (stats[layerName]['total']) + 1\n",
    "\n",
    "\tif 'ema_min' in stats[layerName]:\n",
    "\t\tstats[layerName]['ema_min'] = weighting*(minValue.mean().item()) + (1 - weighting) * stats[layerName]['ema_min']\n",
    "\telse:\n",
    "\t\tstats[layerName]['ema_min'] = weighting*(minValue.mean().item())\n",
    "\n",
    "\tif 'ema_max' in stats[layerName]:\n",
    "\t\tstats[layerName]['ema_max'] = weighting*(maxValue.mean().item()) + (1 - weighting) * stats[layerName]['ema_max']\n",
    "\telse: \n",
    "\t\tstats[layerName]['ema_max'] = weighting*(maxValue.mean().item())\n",
    "\n",
    "\t\tstats[layerName]['min_val'] = stats[layerName]['min']/ stats[layerName]['total']\n",
    "\t\tstats[layerName]['max_val'] = stats[layerName]['max']/ stats[layerName]['total']\n",
    "\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af59c4ae-3014-4e64-af00-c92eeb54b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "\tx = model.flatten(x)\n",
    "\tstats = updateStats(x, stats, 'fc1')\n",
    "\tx = F.relu(model.fc1(x))\n",
    "\t\n",
    "\tstats = updateStats(x, stats, 'fc2')\n",
    "\tx = F.relu(model.fc2(x))\n",
    "\n",
    "\tstats = updateStats(x, stats, 'fc3')\n",
    "\tx = model.fc3(x)\n",
    "\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4865518a-d7ca-482d-a6e4-38cb8b9459e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, testDataLoader):\n",
    "\tmodel.eval()\n",
    "\tstats = {}\n",
    "\twith torch.no_grad():\n",
    "\t\tfor img, lab in testDataLoader:\n",
    "\t\t\timg, lab = img.to(device), lab.to(device)\n",
    "\t\t\tstats = gatherActivationStats(model, img, stats)\n",
    "\n",
    "\tfinal_stats = {}\n",
    "\tfor key, value in stats.items():\n",
    "\t\tfinal_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
    "\treturn final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0986c97-c07a-4974-a2a2-7fc7e3eb2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, min_val=None, max_val=None, num_bits=8, opt='asym'):\n",
    "\t\t#def quantizeTensor(input_tensor, min_val=None, max_val=None, num_bits=4, opt='asym'):\n",
    "        x = quantizeTensor(x, min_val=min_val, max_val=max_val, num_bits=num_bits, opt=opt)\n",
    "        x = dequantizeTensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # straight through estimator\n",
    "        return grad_output, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebd77813-e820-4f02-8feb-64bc5cd71b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantAwareTrainingForward(x, model, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\tx = model.flatten(x)\n",
    "\t\n",
    "\tfc1_weight = model.fc1.weight.data\n",
    "\tmodel.fc1.weight.data = FakeQuantOp.apply(model.fc1.weight.data, None, None, num_bits, opt)\n",
    "\tx = F.relu(model.fc1(x))\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc1')\n",
    "\tif act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc1']['ema_min'], stats['fc1']['ema_max'], num_bits, opt)\n",
    "\n",
    "\tfc2_weight = model.fc2.weight.data\n",
    "\tmodel.fc2.weight.data = FakeQuantOp.apply(model.fc2.weight.data, None, None, num_bits, opt)\n",
    "\tx = F.relu(model.fc2(x))\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc2')\n",
    "\tif act_quant:\n",
    "\t\tx = FakeQuantOp.apply(x, stats['fc2']['ema_min'], stats['fc2']['ema_max'], num_bits, opt)\n",
    "\n",
    "\tfc3_weight = model.fc3.weight.data\n",
    "\tmodel.fc3.weight.data = FakeQuantOp.apply(model.fc3.weight.data, None, None, num_bits, opt)\n",
    "\tx = model.fc3(x)\n",
    "\twith torch.no_grad():\n",
    "\t\tstats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc3')\n",
    "\t\n",
    "#\treturn F.log_softmax(x, dim=1), fc1_weight, fc2_weight, fc3_weight, stats\n",
    "\treturn x, fc1_weight, fc2_weight, fc3_weight, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1801eb1c-e0df-473f-971f-8c8943c04f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQAT(train_dataloader, model, loss_func, optimizer, epoch, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\tmodel.train()\n",
    "\tmax_batch_index = int(np.floor(len(train_data)/batch_size))\n",
    "\tfor batch_index, (image, label) in enumerate(train_dataloader):\n",
    "\t\timage, label = image.to(device), label.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ty = model(image)\n",
    "\t\ty, fc1_weight, fc2_weight, fc3_weight, stats = quantAwareTrainingForward(image, model, stats, act_quant, num_bits, opt)\n",
    "\t\t\n",
    "# \t\tif batch_index is not max_batch_index:\n",
    "\t\t# Recover FP32\n",
    "\t\tmodel.fc1.weight.data = fc1_weight\n",
    "\t\tmodel.fc2.weight.data = fc2_weight\n",
    "\t\tmodel.fc3.weight.data = fc3_weight\n",
    "\n",
    "\t\tloss = loss_func(y, label)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\tif batch_index == max_batch_index:\n",
    "\t\t\tprint(f'Epoch {epoch+1:<3d}: Loss: {loss.item():.2f}', end = '\\t')\n",
    "\t\t\t\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16fb3bab-b0f9-4515-80fe-bbb156cbc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQAT(test_dataloader, model, epoch, loss_func, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\tmodel.eval()\n",
    "\tloss, correct = 0, 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor image, label in test_dataloader:\n",
    "\t\t\timage, label = image.to(device), label.to(device)\n",
    "\t\t\ty, _, _, _, _ = quantAwareTrainingForward(image, model, stats, act_quant, num_bits, opt)\t\n",
    "\t\t\tloss += loss_func(y, label).item()\n",
    "\t\t\tcorrect += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_dataloader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_dataloader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_data)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "951d6389-bfeb-4828-8dec-563dc75dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainQAT():\n",
    "\tmodel = NN().to(device)\n",
    "\tloss_func = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\t\n",
    "\tepochs = 10\n",
    "\tnum_bits = 8\n",
    "\tstats = {}\n",
    "\topt = 'asym'\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tif epoch > 5:\n",
    "\t\t\tact_quant = True\n",
    "\t\telse:\n",
    "\t\t\tact_quant = False\n",
    "\t\t#def trainQAT(train_dataloader, model, loss_func, optimizer, epoch, stats, act_quant=False, num_bits=8, opt='asym'):\n",
    "\n",
    "\t\tstats = trainQAT(train_dataloader, model, loss_func, optimizer, epoch, stats, act_quant, num_bits, opt)\n",
    "\t\ttestQAT(test_dataloader, model, epoch, loss_func, stats, act_quant, num_bits, opt)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "899a93ac-bc99-47a8-a8bc-341fd32988fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  : Loss: 1.61\tAccuracy: 5956/10000 (59.6%)\n",
      "Epoch 2  : Loss: 0.63\tAccuracy: 7821/10000 (78.2%)\n",
      "Epoch 3  : Loss: 0.39\tAccuracy: 8405/10000 (84.0%)\n",
      "Epoch 4  : Loss: 0.28\tAccuracy: 8703/10000 (87.0%)\n",
      "Epoch 5  : Loss: 0.22\tAccuracy: 8817/10000 (88.2%)\n",
      "Epoch 6  : Loss: 0.18\tAccuracy: 8905/10000 (89.0%)\n",
      "Epoch 7  : Loss: 0.24\tAccuracy: 8596/10000 (86.0%)\n",
      "Epoch 8  : Loss: 0.21\tAccuracy: 8736/10000 (87.4%)\n",
      "Epoch 9  : Loss: 0.19\tAccuracy: 8728/10000 (87.3%)\n",
      "Epoch 10 : Loss: 0.24\tAccuracy: 8614/10000 (86.1%)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model, old_stats = mainQAT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aff51b-eec0-4568-ba99-55daa00f9a89",
   "metadata": {},
   "source": [
    "## Rework Forward pass of Linear and Conv Layers to support Quantisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15ac1184-ea39-4ff6-b612-7de355a959c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0304, 0.0195, 0.0000,  ..., 0.0000, 0.0000, 0.0195],\n",
       "        [0.0000, 0.0000, 0.0130,  ..., 0.0195, 0.0000, 0.0260],\n",
       "        [0.0065, 0.0000, 0.0130,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0195,  ..., 0.0000, 0.0000, 0.0195],\n",
       "        [0.0130, 0.0022, 0.0174,  ..., 0.0195, 0.0195, 0.0000],\n",
       "        [0.0000, 0.0325, 0.0304,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1_weight = model.state_dict()['fc1.weight']\n",
    "fc1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c625529d-2395-455e-99f0-08e96b542412",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = quantizeTensor(fc1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67e6b3d8-d10b-4604-b537-c3ae1015609e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0304, 0.0195, 0.0000,  ..., 0.0000, 0.0000, 0.0195],\n",
       "        [0.0000, 0.0000, 0.0130,  ..., 0.0195, 0.0000, 0.0260],\n",
       "        [0.0065, 0.0000, 0.0130,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0195,  ..., 0.0000, 0.0000, 0.0195],\n",
       "        [0.0130, 0.0022, 0.0174,  ..., 0.0195, 0.0195, 0.0000],\n",
       "        [0.0000, 0.0325, 0.0304,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequantizeTensor(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75517c42-e301-488b-9b28-098823b53667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeLayer(qActTuple, layer, stat, numBits=4, opt='asym'):\n",
    "\tW, B = layer.weight.data, layer.bias.data\n",
    "\t\n",
    "\tqWeightTuple = quantizeTensor(W, numBits=numBits, opt=opt)\n",
    "\tqBiasTuple   = quantizeTensor(B, numBits=numBits, opt=opt)\n",
    "\t\n",
    "\tlayer.weight.data = qWeightTuple.qTensor.float()\n",
    "\tlayer.bias.data   = qBiasTuple.qTensor.float()\n",
    "\t\n",
    "\tnextScale, nextZeroPoint = calcScaleZeroPoint(minValue=stat['min'], maxValue=stat['max'], numBits=numBits, opt=opt)\n",
    "\t\n",
    "\tweightScale = qWeightTuple.scale\n",
    "\tweightZeroPoint = qWeightTuple.zeroPoint\n",
    "\t\n",
    "\tif opt == 'asym':\n",
    "\t\tlayer.weight.data = ((qWeightTuple.scale * qActTuple.scale) / nextScale) * (layer.weight.data - qWeightTuple.zeroPoint)\n",
    "\t\tlayer.bias.data = (qBiasTuple.scale / nextScale) * (layer.bias.data - qBiasTuple.zeroPoint)\t\t\n",
    "\t\toAct = layer(qActTuple.qTensor.float() - qActTuple.zeroPoint) + nextZeroPoint\n",
    "\tif opt == 'sym':\n",
    "\t\tlayer.weight.data = ((qWeightTuple.scale * qActTuple.scale) / nextScale) * (layer.weight.data)\n",
    "\t\tlayer.bias.data = (qBiasTuple.scale / nextScale) * (layer.bias.data)\n",
    "\t\toAct = layer(qActTuple.qTensor.float())\n",
    "\t\t\n",
    "\tlayer.weight.data, layer.bias.data = W, B\n",
    "\t\n",
    "\treturn oAct.round(), nextScale, nextZeroPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a930bbf-4853-448c-8f1f-d40d7a3f85b9",
   "metadata": {},
   "source": [
    "## Get Stats for Quantising Activations of Network.\n",
    "\n",
    "This is done by running the network with around 1000 examples and getting the average min and max activation values before and after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b34509-aadf-4e11-9c60-7495f367f90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ae37e-fa48-4b07-99bf-b71a0780733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73fe8b-b293-4f7a-a92c-dd187a4cf2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a4f38-03c2-480f-9939-01adc1397946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
